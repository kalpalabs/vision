{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "_WIKIDATA_RELEVANT_COLUMNS = ['language', 'page_url', 'image_url', 'caption_reference_description', 'caption_attribution_description', 'page_title', 'section_title']\n",
    "_DOWNLOAD_ROOT_URL = \"https://storage.googleapis.com/gresearch/wit/\"\n",
    "_FILENAME = \"wit_v1.{split}.all-{shard}-of-{num_shards}\"\n",
    "_ROOT_OUTPUT_FOLDER = 'wiki_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download, unarchive, and process the sharded data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping in reverse, since test / val sets are much smaller and any issues with the code would be surfaced earlier. \n",
    "for split in ['test', 'val', 'train']:\n",
    "    num_shards = 5 if split in ('test', 'val') else 10\n",
    "    output_dir = os.path.join(_ROOT_OUTPUT_FOLDER, split)\n",
    "\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "        \n",
    "    def get_shard_str(n):\n",
    "        shard_str = str(n)\n",
    "        shard_str = \"0\" * (5 - len(shard_str)) + shard_str\n",
    "        return shard_str\n",
    "\n",
    "    for shard in tqdm(range(num_shards)):\n",
    "        filename = _FILENAME.format(split=split, shard=get_shard_str(shard), num_shards=get_shard_str(num_shards))\n",
    "        zipped_filename = filename + \".tsv.gz\"\n",
    "        url = _DOWNLOAD_ROOT_URL + zipped_filename\n",
    "        \n",
    "        # TODO(pshishodia): Ideally, all of the 3 processes - download, unarchive, trim tsv can be done parallely.\n",
    "        # i.e, when I'm unarchiving second file, I can download the third. so these can be parallelised. \n",
    "        # Offline, I just print these commands in a terminal and use a while loop to check whether I can execute the trim \n",
    "        # step every 10s. \n",
    "        !wget {url}\n",
    "        !pigz -d -f {filename + '.tsv'}  # pigz is gzip with parallelization.\n",
    "        \n",
    "\n",
    "        df = pd.read_csv(filename + \".tsv\", sep='\\t', usecols=_WIKIDATA_RELEVANT_COLUMNS)\n",
    "        df.to_csv(os.path.join(output_dir,  filename + \".csv\"))\n",
    "        os.remove(filename + \".tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine the data\n",
    "This takes <= 5s for val/test, but ~10 minutes for train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in ['test', 'val', 'train']:\n",
    "    output_dir = os.path.join(_ROOT_OUTPUT_FOLDER, split)\n",
    "\n",
    "    # List all csv files in the _ROOT_OUTPUT_FOLDER\n",
    "    csv_files = [os.path.join(output_dir, f) for f in os.listdir(output_dir) if f.endswith('.csv')]\n",
    "    print(f\"{len(csv_files)=}\")\n",
    "\n",
    "    # Combine all csv files into a single dataframe\n",
    "    print(f\"Reading {split=}\")\n",
    "    combined_df = pd.concat([pd.read_csv(f) for f in csv_files])\n",
    "    try:\n",
    "        combined_df = combined_df.drop(columns=['Unnamed: 0'])\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    print(f\"Deduplicating {split=}\")\n",
    "    combined_df = combined_df.groupby(by=['page_url', 'image_url'], as_index=False).first()\n",
    "    \n",
    "    print(f\"Shuffling {split=}\")\n",
    "    combined_df = combined_df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Saving {split=}\")\n",
    "    combined_df.to_csv(os.path.join(_ROOT_OUTPUT_FOLDER, split + \".csv\"), index=False)\n",
    "    \n",
    "    print(f\"================= Completed {split=} ================= \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
